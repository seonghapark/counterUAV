{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import random\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import rnn, rnn_cell\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def windows(data, window_size):\n",
    "    start = 0\n",
    "    while start < len(data):\n",
    "        yield start, start + window_size\n",
    "        start += (window_size / 2)\n",
    "\n",
    "# 각 33개 input, 각 6개 label\n",
    "        \n",
    "def extract_features(file_path, file_label, file_ext=\"*.wav\",bands = 20, frames = 41):\n",
    "    window_size = 512 * (frames - 1)\n",
    "    mfccs = []\n",
    "    labels = []\n",
    "    sound_clip, s = librosa.load(file_path)\n",
    "    #print(type(sound_clip))\n",
    "    if file_label=='other':\n",
    "        label_code = 0\n",
    "    elif file_label=='person':\n",
    "        label_code = 1\n",
    "    elif file_label=='car':\n",
    "        label_code = 2\n",
    "    elif file_label=='drone':\n",
    "        label_code = 3\n",
    "        \n",
    "        \n",
    "    #print(file_label, label_code)\n",
    "    for (start,end) in windows(sound_clip,window_size):\n",
    "        start = int(start)\n",
    "        end = int(end)\n",
    "        if(len(sound_clip[start:end]) == window_size):\n",
    "            signal = sound_clip[start:end]\n",
    "            mfcc = librosa.feature.mfcc(y=signal, sr=s, n_mfcc = bands).T.flatten()[:, np.newaxis].T\n",
    "            mfccs.append(mfcc)\n",
    "            labels.append(label_code)         \n",
    "    features = np.asarray(mfccs).reshape(len(mfccs),frames,bands)\n",
    "    return np.array(features), np.array(labels,dtype = np.int)\n",
    "\n",
    "def one_hot_encode(labels):\n",
    "    n_labels = len(labels)\n",
    "    n_unique_labels = len(np.unique(labels))\n",
    "    one_hot_encode = np.zeros((n_labels,n_unique_labels))\n",
    "    one_hot_encode[np.arange(n_labels), labels] = 1\n",
    "    return one_hot_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_file_path_training = 'C://slice_wav_data/training/'\n",
    "file_list_training = os.listdir(wav_file_path_training)\n",
    "\n",
    "tr_features = []\n",
    "tr_labels = []\n",
    "for f in file_list_training:\n",
    "    features_temp, labels_temp = extract_features(wav_file_path_training + f, f.split(\"_\")[0])\n",
    "    for tr_f in features_temp:\n",
    "        tr_features.append(tr_f)\n",
    "    for tr_l in labels_temp:\n",
    "        tr_labels.append(tr_l)\n",
    "    \n",
    "tr_labels = one_hot_encode(tr_labels)\n",
    "\n",
    "\n",
    "tmp = [[x,y] for x,y in zip(tr_features, tr_labels)]\n",
    "random.shuffle(tmp)\n",
    "tr_features = [n[0] for n in tmp]\n",
    "tr_labels = [n[1] for n in tmp]\n",
    "\n",
    "\n",
    "wav_file_path_test = 'C://slice_wav_data/testing/'\n",
    "file_list_test = os.listdir(wav_file_path_test)\n",
    "\n",
    "ts_features = []\n",
    "ts_labels = []\n",
    "for f in file_list_test:\n",
    "    features_temp, labels_temp = extract_features(wav_file_path_test + f, f.split(\"_\")[0])\n",
    "    for ts_f in features_temp:\n",
    "        ts_features.append(ts_f)\n",
    "    for ts_l in labels_temp:\n",
    "        ts_labels.append(ts_l)\n",
    "    \n",
    "ts_labels = one_hot_encode(ts_labels)\n",
    "\n",
    "tr_features = np.array(tr_features)\n",
    "tr_labels = np.array(tr_labels)\n",
    "\n",
    "ts_features = np.array(ts_features)\n",
    "ts_labels = np.array(ts_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1188, 41, 20)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_arr = np.array(tr_features)\n",
    "temp_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(216, 4)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_arr = np.array(ts_labels) #mfcc를 통해 1188개 트레이닝 데이터, 216개 테스팅 데이터로 변환\n",
    "temp_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "learning_rate = 0.001\n",
    "training_iters = 20000\n",
    "batch_size = 50\n",
    "display_step = 200\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 20\n",
    "n_steps = 41\n",
    "n_hidden = 20\n",
    "n_classes = 4\n",
    "\n",
    "#앞에거는 hidden *2, 뒤에거는 n_input + n_hidden\n",
    "\n",
    "x = tf.placeholder(\"float\", [None, n_steps, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "weight = tf.Variable(tf.random_normal([n_hidden, n_classes]))\n",
    "bias = tf.Variable(tf.random_normal([n_classes]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN(x, weight, bias):\n",
    "    cell = rnn_cell.LSTMCell(n_hidden,state_is_tuple = True)\n",
    "    cell = rnn_cell.MultiRNNCell([cell] * 2, state_is_tuple=True)\n",
    "    output, state = tf.nn.dynamic_rnn(cell, x, dtype = tf.float32)\n",
    "    output = tf.transpose(output, [1, 0, 2])\n",
    "    last = tf.gather(output, int(output.get_shape()[0]) - 1)\n",
    "    return tf.nn.softmax(tf.matmul(last, weight) + bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "prediction = RNN(x, weight, bias)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_f = -tf.reduce_sum(y * tf.log(prediction))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(loss_f)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(prediction,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0, Minibatch Loss= 93.417648, Training Accuracy= 0.36000\n",
      "Iter 100, Minibatch Loss= 65.404861, Training Accuracy= 0.38000\n",
      "Iter 200, Minibatch Loss= 53.215546, Training Accuracy= 0.54000\n",
      "Iter 300, Minibatch Loss= 37.304848, Training Accuracy= 0.68000\n",
      "Iter 400, Minibatch Loss= 34.511467, Training Accuracy= 0.68000\n",
      "Iter 500, Minibatch Loss= 27.462158, Training Accuracy= 0.78000\n",
      "Iter 600, Minibatch Loss= 30.738861, Training Accuracy= 0.74000\n",
      "Iter 700, Minibatch Loss= 23.066650, Training Accuracy= 0.82000\n",
      "Iter 800, Minibatch Loss= 35.500786, Training Accuracy= 0.76000\n",
      "Iter 900, Minibatch Loss= 23.836876, Training Accuracy= 0.78000\n",
      "Iter 1000, Minibatch Loss= 20.383902, Training Accuracy= 0.88000\n",
      "Iter 1100, Minibatch Loss= 25.005852, Training Accuracy= 0.78000\n",
      "Iter 1200, Minibatch Loss= 17.075623, Training Accuracy= 0.82000\n",
      "Iter 1300, Minibatch Loss= 25.335052, Training Accuracy= 0.82000\n",
      "Iter 1400, Minibatch Loss= 18.794292, Training Accuracy= 0.82000\n",
      "Iter 1500, Minibatch Loss= 25.813677, Training Accuracy= 0.84000\n",
      "Iter 1600, Minibatch Loss= 23.867481, Training Accuracy= 0.76000\n",
      "Iter 1700, Minibatch Loss= 27.995352, Training Accuracy= 0.76000\n",
      "Iter 1800, Minibatch Loss= 22.906929, Training Accuracy= 0.84000\n",
      "Iter 1900, Minibatch Loss= 22.623541, Training Accuracy= 0.78000\n",
      "Iter 2000, Minibatch Loss= 14.436973, Training Accuracy= 0.92000\n",
      "Iter 2100, Minibatch Loss= 22.382729, Training Accuracy= 0.78000\n",
      "Iter 2200, Minibatch Loss= 18.703175, Training Accuracy= 0.86000\n",
      "Iter 2300, Minibatch Loss= 12.611551, Training Accuracy= 0.92000\n",
      "Iter 2400, Minibatch Loss= 24.574114, Training Accuracy= 0.76000\n",
      "Iter 2500, Minibatch Loss= 17.219624, Training Accuracy= 0.86000\n",
      "Iter 2600, Minibatch Loss= 19.831366, Training Accuracy= 0.82000\n",
      "Iter 2700, Minibatch Loss= 36.301395, Training Accuracy= 0.76000\n",
      "Iter 2800, Minibatch Loss= 14.028162, Training Accuracy= 0.92000\n",
      "Iter 2900, Minibatch Loss= 18.956516, Training Accuracy= 0.82000\n",
      "Iter 3000, Minibatch Loss= 22.267914, Training Accuracy= 0.78000\n",
      "Iter 3100, Minibatch Loss= 21.524837, Training Accuracy= 0.82000\n",
      "Iter 3200, Minibatch Loss= 20.132196, Training Accuracy= 0.86000\n",
      "Iter 3300, Minibatch Loss= 25.760990, Training Accuracy= 0.76000\n",
      "Iter 3400, Minibatch Loss= 12.435268, Training Accuracy= 0.90000\n",
      "Iter 3500, Minibatch Loss= 16.751934, Training Accuracy= 0.86000\n",
      "Iter 3600, Minibatch Loss= 17.760124, Training Accuracy= 0.86000\n",
      "Iter 3700, Minibatch Loss= 22.841364, Training Accuracy= 0.86000\n",
      "Iter 3800, Minibatch Loss= 16.825205, Training Accuracy= 0.88000\n",
      "Iter 3900, Minibatch Loss= 27.118351, Training Accuracy= 0.76000\n",
      "Iter 4000, Minibatch Loss= 13.059551, Training Accuracy= 0.90000\n",
      "Iter 4100, Minibatch Loss= 22.264786, Training Accuracy= 0.84000\n",
      "Iter 4200, Minibatch Loss= 18.212975, Training Accuracy= 0.82000\n",
      "Iter 4300, Minibatch Loss= 13.484543, Training Accuracy= 0.92000\n",
      "Iter 4400, Minibatch Loss= 21.399654, Training Accuracy= 0.86000\n",
      "Iter 4500, Minibatch Loss= 18.859062, Training Accuracy= 0.80000\n",
      "Iter 4600, Minibatch Loss= 23.196346, Training Accuracy= 0.78000\n",
      "Iter 4700, Minibatch Loss= 11.974628, Training Accuracy= 0.90000\n",
      "Iter 4800, Minibatch Loss= 16.206112, Training Accuracy= 0.84000\n",
      "Iter 4900, Minibatch Loss= 24.958725, Training Accuracy= 0.82000\n",
      "Iter 5000, Minibatch Loss= 13.077679, Training Accuracy= 0.88000\n",
      "Iter 5100, Minibatch Loss= 13.486589, Training Accuracy= 0.88000\n",
      "Iter 5200, Minibatch Loss= 18.050156, Training Accuracy= 0.78000\n",
      "Iter 5300, Minibatch Loss= 10.027229, Training Accuracy= 0.94000\n",
      "Iter 5400, Minibatch Loss= 14.573126, Training Accuracy= 0.82000\n",
      "Iter 5500, Minibatch Loss= 17.360094, Training Accuracy= 0.86000\n",
      "Iter 5600, Minibatch Loss= 9.810600, Training Accuracy= 0.88000\n",
      "Iter 5700, Minibatch Loss= 19.671453, Training Accuracy= 0.82000\n",
      "Iter 5800, Minibatch Loss= 13.654331, Training Accuracy= 0.92000\n",
      "Iter 5900, Minibatch Loss= 18.442371, Training Accuracy= 0.80000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-cf5a9e36fc94>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mbatch_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtr_features\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moffset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0moffset\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mbatch_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtr_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moffset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0moffset\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_f\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;31m#if epoch % display_step == 0:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1332\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    \n",
    "    for itr in range(training_iters):    \n",
    "        offset = (itr * batch_size) % (tr_labels.shape[0] - batch_size)\n",
    "        batch_x = tr_features[offset:offset + batch_size]\n",
    "        batch_y = tr_labels[offset:offset + batch_size]\n",
    "        _, c = session.run([optimizer, loss_f],feed_dict={x: batch_x, y : batch_y})\n",
    "            \n",
    "        #if epoch % display_step == 0:\n",
    "        if itr % 100 == 0:\n",
    "            # Calculate batch accuracy\n",
    "            acc = session.run(accuracy, feed_dict={x: batch_x, y: batch_y})\n",
    "            # Calculate batch loss\n",
    "            loss = session.run(loss_f, feed_dict={x: batch_x, y: batch_y})\n",
    "            print(\"Iter \" + str(itr) + \", Minibatch Loss= \" + \n",
    "                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \n",
    "                  \"{:.5f}\".format(acc))\n",
    "    \n",
    "    print('Test accuracy: ',round(session.run(accuracy, feed_dict={x: ts_features, y: ts_labels}) , 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
