{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# 일관된 출력을 위해 유사난수 초기화\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "%matplotlib inline\n",
    "#그래프를 노트북 안에 그리기 위해서 설정\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# 한글출력\n",
    "# matplotlib.rc('font', family='AppleGothic')  # MacOS\n",
    "matplotlib.rc('font', family='Malgun Gothic')  # Windows\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_input.shape : (1288, 54, 54)\n",
      "test_input.shape : (552, 54, 54)\n"
     ]
    }
   ],
   "source": [
    "################\n",
    "# Layer Params #\n",
    "################\n",
    "n_steps = 54\n",
    "n_inputs = 54\n",
    "n_neurons = 150\n",
    "n_outputs = 4\n",
    "\n",
    "\n",
    "train_input = np.load('rnn_train_input.npy')\n",
    "train_label = np.load('rnn_train_label.npy')\n",
    "\n",
    "test_input = np.load('rnn_test_input.npy')\n",
    "test_label = np.load('rnn_test_label.npy')\n",
    "\n",
    "#true값 중 최대값: 543\n",
    "#true값 중 최소값: 72\n",
    "#543-72=471\n",
    "\n",
    "train_input = (train_input.astype(np.float32).reshape(-1, 54*54) -72) / 471.0  #/ 255.0  # (784,)\n",
    "test_input = (test_input.astype(np.float32).reshape(-1, 54*54) -72) / 471.0  #/ 255.0\n",
    "\n",
    "train_label = train_label.astype(np.int32)\n",
    "test_label = test_label.astype(np.int32)\n",
    "\n",
    "train_input = train_input.reshape([-1, n_steps, n_inputs])\n",
    "test_input = test_input.reshape([-1, n_steps, n_inputs])\n",
    "\n",
    "print('train_input.shape :', train_input.shape)\n",
    "print('test_input.shape :', test_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.35456476, 0.3524416 , 0.35031846, ..., 0.7346072 , 0.7324841 ,\n",
       "        0.7282378 ],\n",
       "       [0.72611463, 0.7239915 , 0.72186834, ..., 0.45010614, 0.44585988,\n",
       "        0.44161358],\n",
       "       [0.4373673 , 0.43312103, 0.42675158, ..., 0.3821656 , 0.38641188,\n",
       "        0.39278132],\n",
       "       ...,\n",
       "       [0.36305732, 0.36093417, 0.35881105, ..., 0.7197452 , 0.7133758 ,\n",
       "        0.7091295 ],\n",
       "       [0.7027601 , 0.6963906 , 0.6878981 , ..., 0.42887473, 0.42675158,\n",
       "        0.42462844],\n",
       "       [0.42250532, 0.42038217, 0.41825902, ..., 0.43312103, 0.4373673 ,\n",
       "        0.43949044]], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input[140]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int32"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_label[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_batch(features, labels, batch_size):\n",
    "    rnd_idx = np.random.permutation(len(features))\n",
    "    n_batches = len(features) // batch_size\n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
    "        batch_x, batch_y = features[batch_idx], labels[batch_idx]\n",
    "        yield batch_x, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "inputs = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "labels = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "# RNN Model\n",
    "basic_cell = tf.nn.rnn_cell.BasicRNNCell(num_units=n_neurons)\n",
    "outputs, states = tf.nn.dynamic_rnn(basic_cell, inputs, dtype=tf.float32)\n",
    "# dense layer\n",
    "logits = tf.layers.dense(states, n_outputs)  # states = outputs[-1]\n",
    "\n",
    "# loss\n",
    "xentropy = tf.reduce_mean(\n",
    "    tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits))\n",
    "\n",
    "################\n",
    "# Train Params #\n",
    "################\n",
    "learning_rate = 0.0001\n",
    "n_epochs = 50\n",
    "batch_size = 250\n",
    "\n",
    "# optimizer\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(xentropy)\n",
    "\n",
    "# metric\n",
    "correct = tf.nn.in_top_k(logits, labels, 1)\n",
    "#evaluation() 함수는 단순히 tf.nn.in_top_kop를 생성한다. 이 op는 자동적으로 참인 레이블이 K most-likely 예측에서 발견되면, 각 모델의 출력을 올바르다고 채점한다.\n",
    "#이 경우에 참인 레이블에 대해 예측이 옳았을 경우만 K의 값을 1로 설정합니다.\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 000 acc_batch : 0.1622, acc_valid : 0.0598 loss_batch : 1.4168\n",
      "epoch : 001 acc_batch : 0.3243, acc_valid : 0.2500 loss_batch : 1.3646\n",
      "epoch : 002 acc_batch : 0.4054, acc_valid : 0.3007 loss_batch : 1.3530\n",
      "epoch : 003 acc_batch : 0.2973, acc_valid : 0.2337 loss_batch : 1.3580\n",
      "epoch : 004 acc_batch : 0.1622, acc_valid : 0.0851 loss_batch : 1.3664\n",
      "epoch : 005 acc_batch : 0.3514, acc_valid : 0.0942 loss_batch : 1.3755\n",
      "epoch : 006 acc_batch : 0.3784, acc_valid : 0.0942 loss_batch : 1.3742\n",
      "epoch : 007 acc_batch : 0.3243, acc_valid : 0.0978 loss_batch : 1.3653\n",
      "epoch : 008 acc_batch : 0.4324, acc_valid : 0.1123 loss_batch : 1.3613\n",
      "epoch : 009 acc_batch : 0.5676, acc_valid : 0.1395 loss_batch : 1.3619\n",
      "epoch : 010 acc_batch : 0.4054, acc_valid : 0.1051 loss_batch : 1.3618\n",
      "epoch : 011 acc_batch : 0.3243, acc_valid : 0.0833 loss_batch : 1.3613\n",
      "epoch : 012 acc_batch : 0.3243, acc_valid : 0.0761 loss_batch : 1.3618\n",
      "epoch : 013 acc_batch : 0.3243, acc_valid : 0.0815 loss_batch : 1.3593\n",
      "epoch : 014 acc_batch : 0.3243, acc_valid : 0.0851 loss_batch : 1.3561\n",
      "epoch : 015 acc_batch : 0.3243, acc_valid : 0.0851 loss_batch : 1.3542\n",
      "epoch : 016 acc_batch : 0.3243, acc_valid : 0.0851 loss_batch : 1.3527\n",
      "epoch : 017 acc_batch : 0.3243, acc_valid : 0.0870 loss_batch : 1.3501\n",
      "epoch : 018 acc_batch : 0.2432, acc_valid : 0.1087 loss_batch : 1.3452\n",
      "epoch : 019 acc_batch : 0.2162, acc_valid : 0.1286 loss_batch : 1.3415\n",
      "epoch : 020 acc_batch : 0.5135, acc_valid : 0.1232 loss_batch : 1.3513\n",
      "epoch : 021 acc_batch : 0.5135, acc_valid : 0.1449 loss_batch : 1.3608\n",
      "epoch : 022 acc_batch : 0.2973, acc_valid : 0.0870 loss_batch : 1.3442\n",
      "epoch : 023 acc_batch : 0.2162, acc_valid : 0.1232 loss_batch : 1.3356\n",
      "epoch : 024 acc_batch : 0.4054, acc_valid : 0.1196 loss_batch : 1.3369\n",
      "epoch : 025 acc_batch : 0.4865, acc_valid : 0.1268 loss_batch : 1.3425\n",
      "epoch : 026 acc_batch : 0.3784, acc_valid : 0.1069 loss_batch : 1.3311\n",
      "epoch : 027 acc_batch : 0.4595, acc_valid : 0.1395 loss_batch : 1.3488\n",
      "epoch : 028 acc_batch : 0.4054, acc_valid : 0.1649 loss_batch : 1.3293\n",
      "epoch : 029 acc_batch : 0.4865, acc_valid : 0.1395 loss_batch : 1.3530\n",
      "epoch : 030 acc_batch : 0.4054, acc_valid : 0.1431 loss_batch : 1.3372\n",
      "epoch : 031 acc_batch : 0.3784, acc_valid : 0.1594 loss_batch : 1.3254\n",
      "epoch : 032 acc_batch : 0.3243, acc_valid : 0.1214 loss_batch : 1.3345\n",
      "epoch : 033 acc_batch : 0.3243, acc_valid : 0.2953 loss_batch : 1.3335\n",
      "epoch : 034 acc_batch : 0.3243, acc_valid : 0.2572 loss_batch : 1.3595\n",
      "epoch : 035 acc_batch : 0.4054, acc_valid : 0.2283 loss_batch : 1.3616\n",
      "epoch : 036 acc_batch : 0.3514, acc_valid : 0.1739 loss_batch : 1.3507\n",
      "epoch : 037 acc_batch : 0.3514, acc_valid : 0.1793 loss_batch : 1.3464\n",
      "epoch : 038 acc_batch : 0.4865, acc_valid : 0.1848 loss_batch : 1.3445\n",
      "epoch : 039 acc_batch : 0.4324, acc_valid : 0.1866 loss_batch : 1.3450\n",
      "epoch : 040 acc_batch : 0.5135, acc_valid : 0.2192 loss_batch : 1.3460\n",
      "epoch : 041 acc_batch : 0.5405, acc_valid : 0.2101 loss_batch : 1.3469\n",
      "epoch : 042 acc_batch : 0.4865, acc_valid : 0.1884 loss_batch : 1.3476\n",
      "epoch : 043 acc_batch : 0.3784, acc_valid : 0.1141 loss_batch : 1.3533\n",
      "epoch : 044 acc_batch : 0.5135, acc_valid : 0.1377 loss_batch : 1.3471\n",
      "epoch : 045 acc_batch : 0.3243, acc_valid : 0.2065 loss_batch : 1.3284\n",
      "epoch : 046 acc_batch : 0.3784, acc_valid : 0.2844 loss_batch : 1.3215\n",
      "epoch : 047 acc_batch : 0.4865, acc_valid : 0.2790 loss_batch : 1.3265\n",
      "epoch : 048 acc_batch : 0.4865, acc_valid : 0.2862 loss_batch : 1.3322\n",
      "epoch : 049 acc_batch : 0.3514, acc_valid : 0.3116 loss_batch : 1.3321\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for i in range(0, len(train_input), batch_size):\n",
    "            start = i\n",
    "            if start + batch_size > len(train_input) - 1:\n",
    "                end = len(train_input) - 1\n",
    "            else:\n",
    "                end = start + batch_size\n",
    "            batch_x = train_input[start:end]\n",
    "            batch_y = train_label[start:end]\n",
    "            sess.run(train_op, feed_dict={inputs: batch_x, labels: batch_y})\n",
    "        acc_batch = accuracy.eval(feed_dict={inputs: batch_x, labels: batch_y})\n",
    "        acc_valid = accuracy.eval(feed_dict={inputs: test_input, labels: test_label})\n",
    "        loss_batch = xentropy.eval(feed_dict={inputs: batch_x, labels: batch_y})\n",
    "        print('epoch : {:03d}'.format(epoch),\n",
    "              'acc_batch : {:.4f}, acc_valid : {:.4f}'.format(acc_batch, acc_valid),\n",
    "              'loss_batch : {:.4f}'.format(loss_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "#accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "#print('Accuracy:', sess.run(accuracy, feed_dict={inputs: test_x, labels: test_y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
