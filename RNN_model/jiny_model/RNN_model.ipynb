{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys, glob  \n",
    "from os.path import isfile, isdir\n",
    "import librosa\n",
    "import librosa.display\n",
    "import librosa.core as core\n",
    "import scipy.io.wavfile as wavfile\n",
    "\n",
    "# To plot pretty figures\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import specgram\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")   # To rid of warnings \n",
    "\n",
    "os_sep = os.sep \n",
    "\n",
    "if sys.platform == 'win32':   # if windows \n",
    "    home = os.path.join('C:', os.sep, 'Users')      \n",
    "elif sys.platform == \"linux\" or sys.platform == \"linux2\" :    \n",
    "    home = os.path.expanduser(\"~\")   # home = os.getenv(\"HOME\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_repo = os.path.join(home, '채진영', 'Desktop', 'counterUAV', 'after_data')\n",
    "wav_data = glob.glob(os.path.join(wav_repo,'**','*.wav'), recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\채진영\\\\Desktop\\\\counterUAV\\\\after_data\\\\20181009_1_100023_person.wav',\n",
       " 'C:\\\\Users\\\\채진영\\\\Desktop\\\\counterUAV\\\\after_data\\\\20181009_1_101055_person.wav',\n",
       " 'C:\\\\Users\\\\채진영\\\\Desktop\\\\counterUAV\\\\after_data\\\\20181009_1_101420_person.wav',\n",
       " 'C:\\\\Users\\\\채진영\\\\Desktop\\\\counterUAV\\\\after_data\\\\20181009_1_101635_person.wav',\n",
       " 'C:\\\\Users\\\\채진영\\\\Desktop\\\\counterUAV\\\\after_data\\\\20181009_1_103752_car.wav',\n",
       " 'C:\\\\Users\\\\채진영\\\\Desktop\\\\counterUAV\\\\after_data\\\\20181009_1_104216_car.wav',\n",
       " 'C:\\\\Users\\\\채진영\\\\Desktop\\\\counterUAV\\\\after_data\\\\20181009_1_104446_car.wav',\n",
       " 'C:\\\\Users\\\\채진영\\\\Desktop\\\\counterUAV\\\\after_data\\\\20181009_2_102248_person.wav',\n",
       " 'C:\\\\Users\\\\채진영\\\\Desktop\\\\counterUAV\\\\after_data\\\\20181009_2_102508_person.wav',\n",
       " 'C:\\\\Users\\\\채진영\\\\Desktop\\\\counterUAV\\\\after_data\\\\20181009_2_102849_person.wav',\n",
       " 'C:\\\\Users\\\\채진영\\\\Desktop\\\\counterUAV\\\\after_data\\\\20181009_3_110458_person.wav',\n",
       " 'C:\\\\Users\\\\채진영\\\\Desktop\\\\counterUAV\\\\after_data\\\\20181009_3_110901_drone.wav',\n",
       " 'C:\\\\Users\\\\채진영\\\\Desktop\\\\counterUAV\\\\after_data\\\\20181009_3_111450_drone.wav',\n",
       " 'C:\\\\Users\\\\채진영\\\\Desktop\\\\counterUAV\\\\after_data\\\\20181009_3_111734_drone.wav',\n",
       " 'C:\\\\Users\\\\채진영\\\\Desktop\\\\counterUAV\\\\after_data\\\\20181016_1_134334_drone.wav',\n",
       " 'C:\\\\Users\\\\채진영\\\\Desktop\\\\counterUAV\\\\after_data\\\\20181016_1_135124_person.wav',\n",
       " 'C:\\\\Users\\\\채진영\\\\Desktop\\\\counterUAV\\\\after_data\\\\20181016_2_134532_car.wav',\n",
       " 'C:\\\\Users\\\\채진영\\\\Desktop\\\\counterUAV\\\\after_data\\\\20181016_3_142025_person.wav',\n",
       " 'C:\\\\Users\\\\채진영\\\\Desktop\\\\counterUAV\\\\after_data\\\\20181016_3_142531_drone.wav',\n",
       " 'C:\\\\Users\\\\채진영\\\\Desktop\\\\counterUAV\\\\after_data\\\\20181114_1_175502_drone.wav',\n",
       " 'C:\\\\Users\\\\채진영\\\\Desktop\\\\counterUAV\\\\after_data\\\\20181114_2_140400_car.wav',\n",
       " 'C:\\\\Users\\\\채진영\\\\Desktop\\\\counterUAV\\\\after_data\\\\20181114_2_141258_car.wav',\n",
       " 'C:\\\\Users\\\\채진영\\\\Desktop\\\\counterUAV\\\\after_data\\\\20181114_2_180324_drone.wav',\n",
       " 'C:\\\\Users\\\\채진영\\\\Desktop\\\\counterUAV\\\\after_data\\\\20181114_3_142450_person.wav']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(wav_data))\n",
    "wav_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def windows(data, window_size):\n",
    "    start = 0\n",
    "    while start < len(data):\n",
    "        yield int(start), int(start + window_size)\n",
    "        start += (window_size / 2)\n",
    "\n",
    "def extract_features(data ,bands = 60, frames = 41):\n",
    "    window_size = 512 * (frames - 1)\n",
    "    mfccs = []\n",
    "    labels = []\n",
    "    sound_clip,s = librosa.load(data)\n",
    "    label = int(data.split('\\\\')[-1].split('_')[1])-1\n",
    "    for (start,end) in windows(sound_clip,window_size):\n",
    "        if(len(sound_clip[start:end]) == window_size):\n",
    "            signal = sound_clip[start:end]\n",
    "            mfcc = librosa.feature.melspectrogram(y=signal, sr=s, n_mels = bands).T.flatten()[:, np.newaxis].T\n",
    "            mfccs.append(mfcc)\n",
    "            labels.append(label)         \n",
    "    features = np.asarray(mfccs).reshape(len(mfccs),frames,bands)\n",
    "    return np.array(features), np.array(labels,dtype = np.int)\n",
    "\n",
    "def one_hot_encode(labels):\n",
    "    n_labels = len(labels)\n",
    "    n_unique_labels = 3\n",
    "    one_hot_encode = np.zeros((n_labels, 3))\n",
    "    one_hot_encode[np.arange(n_labels), labels] = 1\n",
    "    return one_hot_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_wav = len(wav_data) \n",
    "stacked_features = []\n",
    "stacked_labels = []\n",
    "\n",
    "for i in range(n_wav) :\n",
    "    tr_features, tr_labels = extract_features(wav_data[i])\n",
    "    tr_labels = one_hot_encode(tr_labels)\n",
    "    stacked_features.append(tr_features)\n",
    "    stacked_labels.append(tr_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_features = np.array(stacked_features)\n",
    "stacked_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_labels = np.array(stacked_labels)\n",
    "stacked_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = stacked_features\n",
    "y = stacked_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate(X[:20], axis=0)\n",
    "y_train = np.concatenate(y[:20], axis=0)\n",
    "\n",
    "X_val = np.concatenate(X[20:], axis=0)\n",
    "y_val = np.concatenate(y[20:], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1641, 41, 60)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(797, 41, 60)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keras model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TimeDistributed, Bidirectional\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Activation, Flatten, Reshape, LeakyReLU, ReLU\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tensorflow.keras import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import rnn, rnn_cell\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "learning_rate = 0.01\n",
    "batch_size = 50\n",
    "display_step = 200\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 60 \n",
    "n_steps = 41\n",
    "n_hidden = 300\n",
    "n_classes = 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 41, 300)           433200    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 41, 300)           721200    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 41, 300)           721200    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 41, 300)           721200    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 41, 300)           721200    \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 41, 300)           721200    \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 41, 300)           721200    \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 300)               721200    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 3)                 903       \n",
      "=================================================================\n",
      "Total params: 5,482,503\n",
      "Trainable params: 5,482,503\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#build model\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(units=n_hidden, activation='tanh', input_shape=(n_steps, n_input),return_sequences=True))\n",
    "model.add(LSTM(units=n_hidden, activation='tanh', return_sequences=True))\n",
    "model.add(LSTM(units=n_hidden, activation='tanh', return_sequences=True))\n",
    "model.add(LSTM(units=n_hidden, activation='tanh', return_sequences=True))\n",
    "model.add(LSTM(units=n_hidden, activation='tanh', return_sequences=True))\n",
    "model.add(LSTM(units=n_hidden, activation='tanh', return_sequences=True))\n",
    "model.add(LSTM(units=n_hidden, activation='tanh', return_sequences=True))\n",
    "model.add(LSTM(units=n_hidden, activation='tanh', return_sequences=False))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(n_classes, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1641 samples, validate on 797 samples\n",
      "Epoch 1/100\n",
      "1550/1641 [===========================>..] - ETA: 15s - loss: 1.0894 - acc: 0.3716"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-f7b53bb8e832>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m model.fit(X_train, y_train, \n\u001b[0;32m      2\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m           epochs = 100, verbose=1)\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1637\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1638\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1639\u001b[1;33m           validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1641\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    213\u001b[0m           \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 215\u001b[1;33m         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    216\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m           \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2984\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2985\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 2986\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   2987\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2988\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#callbacks_list = [keras.callbacks.EarlyStopping(monitor='acc', patience=50),\n",
    "#                  keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
    "#                                                       factor=0.1, patience=50)]\n",
    "\n",
    "#model.fit(X_train, y_train, \n",
    "#          batch_size=batch_size,validation_data=(X_val, y_val), \n",
    "#          epochs = 100, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
